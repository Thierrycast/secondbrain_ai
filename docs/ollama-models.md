# üìÑ Documenta√ß√£o: Modelfile

O projeto utiliza o Ollama como servidor local de modelos LLM. Para personalizar o comportamento do modelo, usamos arquivos chamados `Modelfile`, que servem como instru√ß√µes de base para o modelo.

---

## üì¶ Estrutura do Modelfile

```Dockerfile
FROM mistral

SYSTEM """
Voc√™ √© um assistente pessoal de IA. Seu papel √© auxiliar com clareza e utilidade...
"""
```

---

## üîç Onde fica

- `Modelfile.example` ‚Üí Exemplo versionado
- `Modelfile` ‚Üí Seu modelo pessoal (n√£o versionado)

O padr√£o √© chamar o modelo com:

```bash
ollama create deva -f Modelfile
```

Voc√™ tamb√©m pode criar vers√µes como:
```bash
ollama create deva-smart -f Modelfile.zephyr
```

---

## ‚úçÔ∏è Instru√ß√µes personalizadas (SYSTEM)

A se√ß√£o `SYSTEM` permite controlar completamente o comportamento do LLM. Exemplos:

### Neutro (default)
```Dockerfile
FROM mistral

SYSTEM """
Voc√™ √© um assistente de IA. Responda com clareza, foco e sem devaneios.
"""
```

### Personalizado
```Dockerfile
FROM mistral

SYSTEM """
Voc√™ √© o Deva, assistente pessoal de Thierry...
Seu papel √© ser √∫til, direto e adapt√°vel.
"""
```

### Comportamento avan√ßado
- Define tom da resposta
- Reage a tipo de contexto (formal/informal)
- Pode restringir ou orientar o que o modelo nunca deve dizer

---

## üõ°Ô∏è Seguran√ßa

- O `Modelfile` NUNCA √© enviado para servidores ‚Äî ele √© usado s√≥ na cria√ß√£o local do modelo Ollama
- Para manter privado, ignore no Git:

```gitignore
Modelfile*
!Modelfile.example
```

---

## ‚úÖ Boas pr√°ticas

- Sempre use `SYSTEM` em blocos longos para controlar o comportamento
- Use `FROM mistral` ou `FROM zephyr` dependendo do estilo desejado
- Teste seu prompt base interativamente ap√≥s gerar o modelo

---

## üß† Dica

Use o comando `ollama show NOME_DO_MODELO` para verificar detalhes do modelo localmente.

---

## ‚öôÔ∏è Sobre desempenho

A performance do assistente depender√° fortemente de dois fatores:

- **Modelo base escolhido**: quanto mais recente e robusto o modelo, maior ser√° a qualidade da resposta. Modelos maiores (como vers√µes otimizadas ou com fine-tuning voltado para conversa√ß√£o) tendem a oferecer melhores resultados, mesmo que demandem mais recursos. O uso de modelos pequenos pode ser suficiente para tarefas simples, mas limitar√° a profundidade e fluidez das respostas.
- **Sua m√°quina**: quanto mais RAM, CPU ou GPU dispon√≠vel, melhor ser√° o desempenho. Computadores mais modestos podem apresentar lentid√£o ao inicializar o modelo ou gerar respostas longas.

---

## ‚úÖ Status

Modelfile estruturado, pronto para ser customizado conforme sua necessidade pessoal, sem depender da nuvem.

